---
layout: talk
type: "Talk"
date: 2023-02-22
name: "Desik Rengarajan"
teaser: "Enhancing Reinforcement Learning Using Data and Structure"
link: "/upcoming/desik_rengarajan"
register: https://us02web.zoom.us/meeting/register/tZ0ud-qqrz0uGtwRj4y-QZm9dvdOj1kgR_x8
---


### Speaker 
Desik Rengarajan is a PhD candidate in the Electrical and Computer Engineering Department at Texas A&M University,working on reinforcement learning. Over the course of his doctoral studies, he has developed algorithms and theories in various fields of reinforcement learning, including multi-armed bandits, multi-agent RL, contextual bandits, online-offline RL, meta RL, and has recently-explored the field of federated RL. He is currently on the job market and welcomes potential opportunities.


## Speaker links:
Website: https://sites.google.com/view/desik-rengarajan/
Google Scholar: https://scholar.google.com/citations?user=ygOY_E4AAAAJ


### Abstract 
In reinforcement learning, reward functions serve as an indirect method of defining the goal of the algorithm. Designing a reward function that accurately captures the task at hand while effectively guiding the learning process can be a difficult challenge, requiring expert domain knowledge and manual fine-tuning. To overcome this, it is often easier to rely on sparse rewards that merely indicate partial or complete task completion. However, this leads to RL algorithms failing to learn an optimal policy in a timely manner due to the lack of fine-grained feedback. During this talk, I will delve into the impact of sparse rewards on reinforcement and meta reinforcement learning and present algorithms that leverage sub-optimal demonstration data to overcome these challenges.

### Papers:
- [Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration](https://arxiv.org/pdf/2202.04628.pdf)
- [Enhanced Meta Reinforcement Learning using Demonstrations in Sparse Reward Environments](https://arxiv.org/pdf/2209.13048.pdf) 
